{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b509e99c",
      "metadata": {
        "id": "b509e99c"
      },
      "source": [
        "## Scipy and Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d84e61",
      "metadata": {
        "id": "81d84e61"
      },
      "source": [
        "In the last notebook we took a tour of the `numpy` and `scipy` stack as it relates to math and statistics. This notebook is going to focus on another subset of `scipy` functionality that is absolutely critical to data analytics. Nearly every statistical algorithm aside from Ordinary Least Squares (OLS) regression cannot be solved algebraically. Because they cannot be solved algebraically, they must be solved **numerically**.\n",
        "\n",
        "### Numeric Optimization\n",
        "\n",
        "Let's start with an example of optimization. Let's say that you know the demand function for tickets to watch a local sports franchise. You can write the inverse demand function as\n",
        "\n",
        "$$ P = 300 - \\frac{1}{2}Q $$\n",
        "\n",
        "and the total cost function as\n",
        "\n",
        "$$ TC = 4000 + 45Q $$\n",
        "\n",
        "In order to choose the right number of tickets to sell, you need to calculate the quantity of tickets that will maximize profits. We can calculate total revenue as $ TR = P \\times Q $, and we can calculate profit as $ \\Pi = TR - TC $. This means that our profit function is\n",
        "\n",
        "$$ \\Pi = 300Q - \\frac{1}{2}Q^2 - 4000 - 45Q $$\n",
        "\n",
        "In order to find the $Q$ associated with the highest achievable level of profit, we can use calculus to find the point at which the rate of change in the profit function is zero ($\\frac{\\partial\\Pi}{\\partial Q}=0$).\n",
        "\n",
        "$$ \\frac{\\partial\\Pi}{\\partial Q} = 300 - Q - 45 = 0 \\implies Q = 255$$\n",
        "\n",
        "So we can **algebraically** solve this particular problem. This isn't always the case. Using `scipy`, we can solve this same problem, as well as many algebraically intractable problems that might be more interesting to us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d805e4",
      "metadata": {
        "id": "c6d805e4"
      },
      "source": [
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/paraboloid.png\" width=\"200\" height=\"200\" />\n",
        "\n",
        "\n",
        "In any optimization problem, we need to find a way to get ourselves to the minimum, and to know when we get there. When we look at the above image, we are able to visually trace the functional shape (looks like a rainbow ice cream cone to me...) and locate the bottom of the function. What we want to do is utilize an algorithm to \"trace\" our way from an arbitrary starting point within a function to the optimal point in that function.\n",
        "\n",
        "In three or fewer dimensions, this is easy. Regressions and statistical models often live in worlds with 100's or 1000's (even millions sometimes) of dimensions. We can't visualize our way to the bottom of those functions!\n",
        "\n",
        "The class of algorithm that is used to solve these problems is called **gradient descent**.\n",
        "\n",
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/gradDesc.png\" width=\"400\" />\n",
        "\n",
        "**Gradient descent** is an algorithm that explores the shape of the function, and determines which direction is most likely to lead to the optimal point. Let's focus on minimization. We want to find our way to the *bottom* of a function, and we can use gradient descent to try to get there. Given any starting point, our goal is to check the direction in which we can move downward most quickly, and start moving in that direction. At some distance from our starting point, we will stop and re-evaluate the direction in which we would like to travel. Are we still heading downhill in the steepest direction? If we aren't, then we need to update our behavior.\n",
        "\n",
        "Our gradient descent algorithm will keep \"looking around\" and moving down until it reaches a point at which it can no longer move \"down\" in any meaningful way. That is the stopping point, and is treated as the optimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af96154",
      "metadata": {
        "id": "0af96154"
      },
      "source": [
        "With an intuitive understanding of how optimization will happen computationally, it's time to learn a bit more about the math and the code that will help us to achieve computational optimization.\n",
        "\n",
        "Consider a function, $f$, with two variables $x$ and $y$. Because there are two input variables in the function, it has two partial derivatives:\n",
        "\n",
        "$$ \\frac{\\partial f}{\\partial x} \\text{ and } \\frac{\\partial f}{\\partial y} $$\n",
        "\n",
        "Each partial derivative tells us how $f$ changes as we move in a particular dimension **all else equal**. The **gradient**, then, is the vector of all partial derivatives of a given function at any point along the function:\n",
        "\n",
        "$$ \\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x} \\\\ \\\\ \\frac{\\partial f}{\\partial y} \\end{matrix} \\right]  $$\n",
        "\n",
        "We can use the gradient to determine the linear approximation of a function at any given point. Think about the gradient as the mathematical representation of the slope and direction of a hill you are hiking on. If we know the gradient, we know which way is down. As we continue to calculate gradients while walking, we can continue to ensure that we will walk downhill until we reach the bottom of the hill.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3fefac",
      "metadata": {
        "id": "6b3fefac"
      },
      "source": [
        "The steps of our gradient descent function will be the following:\n",
        "\n",
        "- Evaluate the gradient of the function\n",
        "- Find the direction of steepest descent\n",
        "- Determine how far to move in that direction\n",
        "- Move to new point\n",
        "- Reevaluate the gradient\n",
        "- Stop moving when gradient is within a margin of error from 0\n",
        "\n",
        "Let's try to implement gradient descent by solving our old profit maximization problem computationally. The very first thing that we need to do is write a Python function that will represent our mathematical function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c641a5e",
      "metadata": {
        "id": "3c641a5e"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    return tr - tc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1c0514",
      "metadata": {
        "id": "ed1c0514"
      },
      "source": [
        "This function will allow us to calculate profit at any output level based on our assumed total costs and demand curve. With this function, we can quickly calculate the gradient (in this case, just a simple derivative because our function is univariate) by calculating profit at two nearby points, and dividing by the horizontal distance between those points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef8c0ac",
      "metadata": {
        "id": "4ef8c0ac"
      },
      "outputs": [],
      "source": [
        "# Gradient at q=200\n",
        "\n",
        "(profit(201) - profit(199))/2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e219ac5",
      "metadata": {
        "id": "5e219ac5"
      },
      "source": [
        "    55.0\n",
        "\n",
        "\n",
        "\n",
        "Thus, a one unit increase in output at $Q=200$ results in a $55 increase in profits. This is cool, but it isn't enough for us to find the point of maximum profit (the optimal point). For that, we will need to calculate LOTS of gradients in order to move along the function until we cannot increase profits any further.\n",
        "\n",
        "Fortunately for us, `scipy` comes with optimization tools that will do all of the heavy lifting of the \"search\" for the optimal point. All that we need to do is frame our question algorithmically, and let `scipy` do the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edaa6713",
      "metadata": {
        "id": "edaa6713"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4f0c01",
      "metadata": {
        "id": "6b4f0c01"
      },
      "source": [
        "We start by importing the `minimize` function from `scipy.optimize`. Hang on! Weren't we working on a MAXIMIZATION problem?? What are we doing here?\n",
        "\n",
        "Maximization and minimization are the **same thing**. To maximize a function, you can multiply that function by `-1` and then calculate the minimum of the new \"upside-down\" function. It is functionally equivalent. So, in computational optimization, we always minimize.\n",
        "\n",
        "### Prepping for optimization\n",
        "\n",
        "As we prepare to optimize, there are two common problems with our function that we may need to resolve:\n",
        "\n",
        "1) When using `minimize` we can only pass an array of inputs, so we have to be careful to write our function accordingly\n",
        "2) Our problem is concave, and so has a maximum\n",
        "\t- We need to restate it as a minimization problem\n",
        "\n",
        "Problem 1 does not apply, since our function in univariate. In order to make our problem a minimization problem, we can flip our profit maximization function. We will simply return -1 * profit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c950291a",
      "metadata": {
        "id": "c950291a"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    pi =  tr - tc # didn't name it profit because that is our function's name! Don't want to clutter our name space!\n",
        "    return -1*pi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b1a329",
      "metadata": {
        "id": "56b1a329"
      },
      "source": [
        "### Making the call to `minimize`\n",
        "\n",
        "Now that our function is ready, it is time to minimize! The `minimize` function takes two arguments:\n",
        "1. Our function that we want to optimize\n",
        "2. A starting guess (as a vector)\n",
        "\n",
        "Let's give it a try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d46c4fa",
      "metadata": {
        "id": "7d46c4fa"
      },
      "outputs": [],
      "source": [
        "res = minimize(profit, [0]) # provide function and starting inputs\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61532439",
      "metadata": {
        "id": "61532439"
      },
      "source": [
        "          fun: -28512.499999980355\n",
        "     hess_inv: array([[1.00000175]])\n",
        "          jac: array([0.])\n",
        "      message: 'Optimization terminated successfully.'\n",
        "         nfev: 21\n",
        "          nit: 3\n",
        "         njev: 7\n",
        "       status: 0\n",
        "      success: True\n",
        "            x: array([255.00019821])\n",
        "\n",
        "\n",
        "\n",
        "That's it! No calculus, no searching, no checking gradients manually. `minimize` simply takes our function and our starting guess and brings us back the optimal choice. We get lots of information stored in the attributes of the `res` object:\n",
        "\n",
        "- `fun` provides the value of the function (this is -1 times the profit level at the optimal output in our example)\n",
        "- `hess_inv` and `jac` are measures of gradient and are used to determine how far to go and in which direction\n",
        "- `message` should be self-explanatory\n",
        "- `nfev` is the number of times the function (in this case `profit`) was evaluated during the search\n",
        "- `nit` is the number of iterations it took to find the optimum\n",
        "- `njev` is the number of times the Jacobian was estimated\n",
        "- `status` is a code associated with the `message` and `success` atrributes\n",
        "- `success` tells you whether or not an optimum was found (sometimes it cannot be easily found!)\n",
        "- `x` probably the most important attribute. This tells us the optimal input value (in our case $Q$), or set of values depending on our function. In a regression context, this could represent the fitted coefficients!\n",
        "\n",
        "Going forward, you will realize (especially because so many of them print the output as they optimize) just how many libraries in Python use this minimize function behind the scenes. It is used in `statsmodels`, `sklearn`, and many other high-profile libraries! Now that you know where it really happens (in `scipy`!), you'll be better able to troubleshoot the problems that will inevitably arise as you use statistical models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4035bfcd",
      "metadata": {
        "id": "4035bfcd"
      },
      "source": [
        "**Solve-it!**\n",
        "\n",
        "In this lesson we learned about optimization using SciPy. For the assignment this week, I would like you to build off of your `RegressionModel` class. You will add a Logistic Regression (Logit) method to your class, so that when the `regression_type` parameter is `logit`, Logistic Regression Results are returned.\n",
        "\n",
        "Your job is to create the following functionality within your class object:\n",
        "- a method (call it `logistic_regression`) that estimates the results of logistic regression using your `x` and `y` data frames, and using a likelihood function and gradient descent (DO NOT USE PREBUILT REGRESSION FUNCTIONS).\n",
        "    - You need to write a function to calculate the Log-likelihood of your model\n",
        "    - You need to implement gradient descent to find the optimal values of beta\n",
        "    - You need to use your beta estimates, the model variance, and calculate the standard errors of the coefficients, as well as Z statistics and p-values\n",
        "    - the results should be stored in a dictionary named `results`, where each variable name (including the intercept if `create_intercept` is `True`) is the key, and the value is another dictionary, with keys for `coefficient`, `standard_error`, `z_stat`, and `p_value`. The coefficient should be the log odds-ratio (which takes the place of the coefficients in OLS)\n",
        "- a method called `fit_model` that uses the `self.regression_type` attribute to determine whether or not to run an OLS or Logistic Regression using the data provided. This method should call the correct regression method.\n",
        "- an updated method (call it `summary`) that presents your regression results in a table\n",
        "    - Columns should be: Variable name, Log odds-ratio value, standard error, z-statistic, and p-value, in that order.\n",
        "    - Your summary table should have different column titles for OLS and Logistic Regressions! (think if statement...)\n",
        "\n",
        "You only need to define the class. My code will create an instance of your class (be sure all the names match these instructions, and those from last week!), and provide data to run a regression. I will provide the same data to you, so that you can experiment and make sure that your code is functioning properly.\n",
        "\n",
        "NOTE: I have created a [primer on Logistic regression](https://github.com/dustywhite7/Econ8320/blob/master/SlidesPDF/9-2%20-%20Logit%20Primer.pdf) to go with this assignment. See the Github slidesPDF folder\n",
        "\n",
        "Put the code that you would like graded in the cell labeled `#si-exercise`. I recommend copying your code from the last assignment (in chapter 9 about linear regression and `numpy`), and continuing from there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "6a146533",
      "metadata": {
        "id": "6a146533"
      },
      "outputs": [],
      "source": [
        "#si-exercise\n",
        "import numpy as np\n",
        "import scipy.stats as stat\n",
        "import pandas as pd\n",
        "import scipy.optimize as opt\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "class RegressionModel(object):\n",
        "    def __init__(self, x, y, create_intercept=True, regression_type='ols'):\n",
        "        if isinstance(x, pd.DataFrame):\n",
        "            self.x = x\n",
        "        else:\n",
        "            raise RuntimeError(\"Matrix 'x' is not a DataFrame.\")\n",
        "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
        "            self.y = y\n",
        "        else:\n",
        "            raise RuntimeError(\"Matrix 'y' is not a DataFrame or Series.\")\n",
        "        if isinstance(create_intercept, bool):\n",
        "            self.create_intercept = create_intercept\n",
        "            if self.create_intercept:\n",
        "                self.add_intercept()\n",
        "        else:\n",
        "            raise RuntimeError(\"Parameter 'create_intercept' must be a boolean value.\")\n",
        "        if isinstance(regression_type, str):\n",
        "            if regression_type==\"ols\":\n",
        "                self.regression_type=regression_type\n",
        "            elif regression_type=='logit':\n",
        "                self.regression_type=regression_type\n",
        "            else:\n",
        "                raise RuntimeError(\"Only OLS and Logistic regressions ('ols' or 'logit', respectively) are supported\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Parameter 'regression_type' must be a string with value 'ols' or 'logit'.\")\n",
        "\n",
        "    def add_intercept(self):\n",
        "        self.x = self.x.assign(intercept=pd.Series([1]*np.shape(self.x)[0]))\n",
        "\n",
        "    def ols_regression(self):\n",
        "        x = self.x\n",
        "        y = self.y\n",
        "        n, k = np.shape(x)\n",
        "        beta = np.dot(np.linalg.solve(x.T.dot(x), np.eye(k)), x.T.dot(y))\n",
        "        eps = y - x.dot(beta)\n",
        "        shat = eps.T.dot(eps)/(n-k)\n",
        "        covar = shat * np.linalg.solve(x.T.dot(x), np.eye(k))\n",
        "        var = np.diag(covar)\n",
        "        serror = np.asarray([np.sqrt(i) for i in var])\n",
        "        tstat = np.asarray([i[1]/serror[i[0]] for i in enumerate(beta)])\n",
        "        pval = stat.t.sf(tstat, n-k)\n",
        "\n",
        "        self.results = dict()\n",
        "\n",
        "        for i in enumerate(self.x.columns):\n",
        "            self.results[i[1]] = {\n",
        "                    'coefficient' : beta[i[0]],\n",
        "                    'standard_error' : serror[i[0]],\n",
        "                    't_stat' : tstat[i[0]],\n",
        "                    'p_value' : pval[i[0]]\n",
        "                    }\n",
        "\n",
        "    def logistic_transform(self, x, beta):\n",
        "        return 1 / (1 + np.exp(-np.dot(x, beta)))\n",
        "\n",
        "    def logistic_log_likelihood(self, beta):\n",
        "        p = self.logistic_transform(self.x, beta)\n",
        "        ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "        return -ll\n",
        "\n",
        "    def logistic_regression(self):\n",
        "        beta_init = np.zeros(self.x.shape[1])\n",
        "\n",
        "        result = minimize(self.logistic_log_likelihood, beta_init)\n",
        "        beta_hat = result.x\n",
        "\n",
        "        # Calculate model variance\n",
        "        n = self.x.shape[0]\n",
        "        p_hat = self.logistic_transform(self.x, beta_hat)\n",
        "        model_variance = n * np.mean(p_hat * (1 - p_hat))\n",
        "\n",
        "        # Estimate variance-covariance matrix\n",
        "        cov_beta_hat = np.linalg.inv(np.dot(self.x.T, self.x))\n",
        "        cov_beta_hat *= model_variance\n",
        "\n",
        "        # Calculate standard errors and z-statistics\n",
        "        se = np.sqrt(np.diag(cov_beta_hat))\n",
        "        z_stat = beta_hat / se\n",
        "\n",
        "        # Store results\n",
        "        self.results = {}\n",
        "        for i, col in enumerate(self.x.columns):\n",
        "            self.results[col] = {\n",
        "                'coefficient': beta_hat[i],\n",
        "                'standard_error': se[i],\n",
        "                'z_stat': z_stat[i],\n",
        "                'p_value': 2 * (1 - norm.cdf(np.abs(z_stat[i])))\n",
        "            }\n",
        "    def fit_model(self):\n",
        "        if self.regression_type == 'ols':\n",
        "            self.ols_regression()\n",
        "        elif self.regression_type == 'logit':\n",
        "            self.logistic_regression()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid regression type.\")\n",
        "\n",
        "    def summary(self):\n",
        "          if self.regression_type == 'ols':\n",
        "              print(\"OLS Regression Results:\")\n",
        "              print(\"Variable Name\".ljust(25) + \"| Coefficient\".ljust(15) + \"| Standard Error\".ljust(17) + \"| T-Statistic\".ljust(15) + \"| P-Value\".ljust(15) + \"\\n\" + \"-\"*85)\n",
        "          elif self.regression_type == 'logit':\n",
        "              print(\"Logistic Regression Results:\")\n",
        "              print(\"Variable Name\".ljust(25) + \"| Coefficient\".ljust(15) + \"| Standard Error\".ljust(17) + \"| Z-Statistic\".ljust(15) + \"| P-Value\".ljust(15) + \"\\n\" + \"-\"*85)\n",
        "\n",
        "          for var_name, result in self.results.items():\n",
        "              coefficient = \"{:.6f}\".format(result['coefficient'])\n",
        "              standard_error = \"{:.6f}\".format(result['standard_error'])\n",
        "              z_statistic = \"{:.6f}\".format(result['z_stat'])\n",
        "              p_value = \"{:.6f}\".format(result['p_value'])\n",
        "\n",
        "              print(f\"{var_name.ljust(25)} | {coefficient.ljust(15)} | {standard_error.ljust(17)} | {z_statistic.ljust(15)} | {p_value.ljust(15)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "g8O3uOFkjhps",
        "outputId": "007ac9d6-9dba-4baa-f642-6e7989841a45"
      },
      "id": "g8O3uOFkjhps",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0e12f4e-c1f6-44e0-8601-88bda3139dc5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0e12f4e-c1f6-44e0-8601-88bda3139dc5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving assignment8Data.csv to assignment8Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('assignment8Data.csv')\n",
        "\n",
        "# Selecting specific variables for the test case\n",
        "x = data.loc[:100, ['sex', 'age', 'educ']]\n",
        "y = data.loc[:100, 'white']\n",
        "\n",
        "# Instantiate the RegressionModel class with logistic regression\n",
        "regression_model = RegressionModel(x, y, create_intercept=True, regression_type='logit')\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "results = regression_model.fit_model()\n",
        "\n",
        "# Print summary\n",
        "regression_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ0uc3nPjyuZ",
        "outputId": "45c0984f-0517-4fb0-cff2-55b2855944bd"
      },
      "id": "AQ0uc3nPjyuZ",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Results:\n",
            "Variable Name            | Coefficient  | Standard Error | Z-Statistic  | P-Value      \n",
            "-------------------------------------------------------------------------------------\n",
            "sex                       | -1.122917       | 0.395672          | -2.838001       | 0.004540       \n",
            "age                       | -0.007013       | 0.010773          | -0.650956       | 0.515075       \n",
            "educ                      | -0.046486       | 0.100415          | -0.462943       | 0.643405       \n",
            "intercept                 | 5.735449        | 1.120065          | 5.120642        | 0.000000       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x) - f0\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x) - f0\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x) - f0\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n",
            "  df = fun(x) - f0\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
            "<ipython-input-54-052d97b59338>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  ll = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}